{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "MmE8T4whd1U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Linear regression is a method to model the relation between values:\n",
        "\n",
        "\n",
        "*   Input data $A = \\begin{bmatrix}\n",
        "a_1 & a_2 & \\cdots & a_n\n",
        "\\end{bmatrix}$ where columns $a_i$'s are different data features;\n",
        "*   Output data $y$ which is a column vector.\n",
        "\n",
        "The linear regression model assumes that the output $y$ is a linear function in terms of the input variables. So basically it can be formulated into a linear system\n",
        "\n",
        "$$y = Ax$$\n",
        "\n",
        "where the vector $x$ is a vector of coefficients (or weights of this linear model) which reveals how output $y$ relates *linearly* with the input $A$, and it is the vector we would want to solve.\n",
        "\n",
        "You may want to say, well this is a linear system, why not just solve it? The fact is that, in general, there are more equations than the number of unknowns (data features, number of columns of input $A$), which means that the linear system we have is often \"overdetermined\", and inconsistent.\n",
        "\n",
        "The way to achieve the \"best approximation\" of the solution is when the values for $x$ in the linear model minimize the square error: find $x$ such that the norm\n",
        "\n",
        "$$||y - Ax||^2 = (y - Ax) \\cdot (y - Ax).$$\n",
        "\n",
        "We will see in the Section of least square problems that the least square solution $x$ to a generally inconsistent linear model $Ax = y$ is unique when columns of $A$ are linearly independent. The key point to get such best approximation $x$ is to modify the linear system as the following *consistent* linear system:\n",
        "\n",
        "$$\\widehat{y} = \\text{proj}_{\\text{Col}(A)} y = Ax$$\n",
        "\n",
        "and solve it. It is the same as solving the normal equation:\n",
        "\n",
        "$$(A^T A)x = A^T y \\implies x = (A^T A)^{-1}A^T y.$$\n",
        "\n",
        "The presense of the inverse $(A^T A)^{-1}$ is potentially the challanging part.\n",
        "\n"
      ],
      "metadata": {
        "id": "fylWPgr6bYLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "Let consider a simple linear regression problem where there is a single feature, namely, $A$ and $y$ are both column vectors. We aim to find a scalar $x \\in \\mathbb{R}$ which describes the linear relation between $A$ and $y$ the best."
      ],
      "metadata": {
        "id": "8pKII4o2dAMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define input and output data\n",
        "data = np.array([\n",
        "    [1.03, 3.46],\n",
        "    [2.32, 5.78],\n",
        "    [2.98, 9.98],\n",
        "    [3.67, 10.56],\n",
        "    [4.39, 14.09],\n",
        "    [5.64, 15.21]\n",
        "])\n",
        "A, y = data[:, 0], data[:, 1]\n",
        "A = A.reshape(len(A), 1)\n",
        "plt.scatter(A, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BvbLG3o6bOKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse"
      ],
      "metadata": {
        "id": "YtG4bAOXbfNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest approach is to solve its normal equation directly as mentioned above:\n",
        "\n",
        "$$x = (A^TA)^{-1}A^Ty.$$"
      ],
      "metadata": {
        "id": "SafWvN4MjGde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solve the normal equation directly\n",
        "# least square solution\n",
        "x = la.inv(A.T @ A) @ A.T @ y\n",
        "print(x)\n",
        "# use x to predict the output\n",
        "yhat = A @ x\n",
        "# plot the graph with the predicted line\n",
        "plt.scatter(A, y)\n",
        "plt.plot(A, yhat, 'r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yM6mhbx_bvY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QR decomposition\n",
        "\n",
        "Another approach is using QR decomposition of $A$ when columns of $A$ are linearly independent.\n",
        "\n",
        "Recall how we obtain the QR factorization for a tall matrix $A$ whose columns are linearly independent. First we apply the Gram-Schmidt process to the column vectors of $A$ to get an orthonormal basis for the column space $\\text{Col}(A)$. We denote by $Q$ the matrix whose columns are formed by the orthonormal basis elements. Then the QR factorization of $A$ is\n",
        "\n",
        "$$A = QR$$\n",
        "\n",
        "where $R$ is a upper triangular matrix, which is a square matrix, invertible, with strictly positive main diagonal entries.\n",
        "\n",
        "Using the decomposition, to solve the least square problem $y = Ax$ we obtain\n",
        "\n",
        "$$Q^T y = Q^TQR x \\implies x = R^{-1}Q^T y$$\n",
        "\n",
        "since $Q$ is an orthogonal matrix, $Q^TQ = I$, and $R$ is invertible. Note that the left-hand-side $Q^T y = \\text{proj}_{\\text{Col}(A)} y$."
      ],
      "metadata": {
        "id": "N0rE3milbwHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use QR decomposition\n",
        "Q, R = la.qr(A)\n",
        "x_QR = la.inv(R) @ Q.T @ y\n",
        "print(x_QR)\n",
        "# use x_QR to predict the output\n",
        "yhat = A @ x_QR\n",
        "# plot the graph with the predicted line\n",
        "plt.scatter(A, y)\n",
        "plt.plot(A, yhat, 'r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFBoy2Jib2NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVD and Pseudo-inverse\n",
        "\n",
        "Let $A$ be an $n \\times m$ real matrix. The SVD (singular value decomposition) of $A$ is\n",
        "\n",
        "$$A = U \\Sigma V^T$$\n",
        "\n",
        "where\n",
        "\n",
        "*   $U$ is a square matrix of size $n \\times n$;\n",
        "*   $\\Sigma$ is an $n \\times m$ matrix which is diagonal;\n",
        "*   $V^T$ is another square matrix of size $m \\times m$;\n",
        "*   Both $U$ and $V$ are orthogonal matrices. The diagonal entries of $\\Sigma$ are called *singular values* which are always nonnegative.\n",
        "\n",
        "By viewing $A = U\\Sigma V^T$ as a linear transformation and a composition of three linear transformations, it can be considered as rotation (represented by $V^T$), followed by stretching by singular values (represented by $\\Sigma$), and then followed by another rotation (represented by $U$).\n",
        "\n",
        "Note that $A$ has to have linearly independent column vectors in order to admit a QR factorization, but there is no requirement on $A$ to get a singular value decomposition. Thus SVD is more general and stable.\n",
        "\n",
        "We will use SVD to find inverse of matrix $A^T A$ (if exists, with size $m \\times m$) and take advantage of $U^T = U^{-1}$ if $U$ is a square orthogonal matrix. Assume SVD of $A^T A$ is\n",
        "\n",
        "$$A^T A = U \\Sigma V^T \\implies (A^T A)^{-1} = V\\Sigma^{-1} U^T,$$\n",
        "\n",
        "where since $\\Sigma$ is square diagonal with main diagonal entries positive,\n",
        "\n",
        "$$\\Sigma^{-1} = \\begin{bmatrix}\n",
        "1/s_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1/s_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & 1/s_m\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "In general, not all singular values are strictly positive, which means that $\\Sigma$ may not be invertible. This happens when the square matrix $A^TA$ is not invertible. The most straightforward way to remedy this situation is the following. Suppose\n",
        "\n",
        "$$\\Sigma = \\begin{bmatrix}\n",
        "s_1 & & & & & \\\\\n",
        " & \\ddots & & & & \\\\\n",
        " & & s_k & & & \\\\\n",
        " & & & 0 & & \\\\\n",
        " & & & & \\ddots & \\\\\n",
        " & & & & & 0\n",
        "\\end{bmatrix},$$\n",
        "\n",
        "where $s_1,\\cdots,s_k$ are strictly positive.\n",
        "We define its *pseudo-inverse*, denoted by $\\Sigma^\\dagger$ as\n",
        "\n",
        "$$\\Sigma^\\dagger = \\begin{bmatrix}\n",
        "1/s_1 & & & & & \\\\\n",
        " & \\ddots & & & & \\\\\n",
        " & & 1/s_k & & & \\\\\n",
        " & & & 0 & & \\\\\n",
        " & & & & \\ddots & \\\\\n",
        " & & & & & 0\n",
        "\\end{bmatrix},$$\n",
        "\n",
        "Thus when $(A^TA)$ is not invertible, we can still take its pseudo-inverse, together with SVD, it is given by\n",
        "\n",
        "$$(A^T A)^\\dagger = V \\Sigma^\\dagger U^T,$$\n",
        "\n",
        "and the solution to the normal equation is\n",
        "$$x = (A^T A)^\\dagger A^T y.$$"
      ],
      "metadata": {
        "id": "L1G8e6Fob2zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use singular value decomposition (SVD)\n",
        "# the function la.pinv is a built-in computing pseudo-inverse of a matrix\n",
        "x_SVD = la.pinv(A.T @ A) @ A.T @ y\n",
        "print(x_SVD)\n",
        "# use x_SVD to predict the output\n",
        "yhat = A @ x_SVD\n",
        "# plot the graph with the predicted line\n",
        "plt.scatter(A, y)\n",
        "plt.plot(A, yhat, 'r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CigU0wImb_g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "\n",
        "Let $L_1, L_2, \\cdots, L_7$ be seven lines in $\\mathbb{R}^6$. Find a point $x \\in \\mathbb{R}^6$ which minimizes the sum of distances to all the seven lines $L_1, L_2,\\cdots, L_7$.\n",
        "\n",
        "### Setup\n",
        "\n",
        "1.   We all know that two distinct points define a line. To describe a line $L$ in $\\mathbb{R}^6$, let $v$ and $w$ be two distinct points on $L$. Then we have a direction vector of the line $L$ given by\n",
        "\n",
        "$$r = \\dfrac{v - w}{||v - w||}.$$\n",
        "\n",
        "2.   There is matrix $(I - rr^T)$ which is called the projection matrix. For any vector $p \\in \\mathbb{R}^6$,\n",
        "\n",
        "$$(I - rr^T)p = p - r(r^Tp) = p - (p \\cdot r)r = p - \\text{proj}_{r}p$$\n",
        "\n",
        "which means that the matrix $(I - rr^T)$ projects each vector $p$ to the subspace $W$ which is orthogonal to $L$.\n",
        "\n",
        "3.   Using the projection matrix $(I - rr^T)$, the intersection of $L$ with the subspace $W$ is given by the projection of $v$ (or $w$) onto $W$:\n",
        "\n",
        "$$q = (I - rr^T)v.$$\n",
        "\n",
        "4.   Thus for a point $x \\in \\mathbb{R}^6$, its distance to the line $L$ can be computed as distance of its projection to $W$ to the intersection point $q$. In other words, we express the distance between line $L$ and point $p$, as length of line segments lying inside $W$ with endpoints $q$ and $(I - rr^T)x$:\n",
        "\n",
        "$$||q - (I - rr^T)x||.$$\n",
        "\n",
        "Applying to a set of seven lines $L_1,\\cdots,L_7$, we want to find point $x \\in \\mathbb{R}^6$ which minimizes\n",
        "\n",
        "$$\\sum_{i = 1}^7 ||q_i - (I - r_ir_i^T)x||^2$$\n",
        "\n",
        "which can be written as a quadratic form in terms of the vector $x$:\n",
        "\n",
        "$$\\sum_{i = 1}^7 ||q_i - (I - r_ir_i^T)x||^2 = x^T A x - 2b^T x + c$$\n",
        "\n",
        "where\n",
        "\n",
        "*   $A = \\sum_{i = 1}^7 (I - rr^T)$ is of size $7 \\times 7$;\n",
        "*   $b = \\sum_{i = 1}^7 q_i$ is a vector;\n",
        "*   $c = \\sum_{i = 1}^7 q_i^T q_i$ is a scalar.\n",
        "\n",
        "Since $x$ which minimizes $x^T A x - 2b^T x + c$ is precisely given by the least square solution to $Ax = b$,\n",
        "\n",
        "**Use the above three methods (directly solving normal equation, QR decomposition, SVD) to find such minimizer $x$.**\n",
        "\n"
      ],
      "metadata": {
        "id": "JLrZwT4vcAsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "# generate seven lines in R^6\n",
        "rng = np.random.default_rng(seed=3264)\n",
        "L = rng.uniform(low=-10, high=10, size=(7, 2, 6))  # (lines, 2 points v/w, coordinates)\n",
        "# seven pairs of points to define seven lines\n",
        "V, W = L[:, 0], L[:, 1]\n",
        "# seven direction vectors of seven lines\n",
        "R = V - W\n",
        "R = R / la.norm(R, axis=1, keepdims=True)\n",
        "\n",
        "# seven projection matrices\n",
        "RRT = R[:, :, None] @ R[:, None, :]  # or np.einsum('ij,ik->ijk', R, R)\n",
        "projM = np.eye(6) - RRT\n",
        "# intersection points of lines with their corresponding orthogonal subspaces\n",
        "Q = np.einsum('ijk,ik->ij', projM, V)  # or (projM @ V[:, :, None])[..., 0]\n",
        "print(Q.shape)\n",
        "\n",
        "# get A, b\n",
        "A = np.sum(projM, axis=0)\n",
        "b = np.sum(Q, axis=0)\n",
        "print(A.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "id": "23pUXhhcZdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### solution ####\n",
        "# QR factorization\n",
        "# x_QR = ?\n",
        "\n",
        "\n",
        "#### solution ####\n",
        "\n",
        "#### solution ####\n",
        "# psuedo-inverse\n",
        "# x_SVD = ?\n",
        "\n",
        "\n",
        "#### solution ####"
      ],
      "metadata": {
        "id": "pV6Ifo74b3t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncommet and run the following when you are done\n",
        "# print(np.allclose(x_QR, x_SVD))  # compare solutions from both approaches"
      ],
      "metadata": {
        "id": "-RmVrSKNbcdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}